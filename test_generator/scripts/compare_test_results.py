#!/usr/bin/env python3
"""Compare two pytest JUnit XML reports and detect regressions.

Reads two JUnit XML files (generated by pytest --junitxml=report.xml),
compares test counts, pass/fail rates, and execution times.

Usage:
    python compare_test_results.py old_results.xml new_results.xml
    python compare_test_results.py old.xml new.xml --output report.md
    python compare_test_results.py old.xml new.xml --max-time-ratio 1.5

Exit codes:
    0 - No regression (new results match or improve on old)
    1 - Regression detected (new failures or missing tests)
    2 - Input error (file not found, invalid XML)

Note: This is a CLI tool - print() statements are intentional for user output.
"""
# ruff: noqa: T201

import sys
import xml.etree.ElementTree as ET
from pathlib import Path


def load_junit_xml(filepath: str) -> dict:
    """Load and parse a JUnit XML report from pytest."""
    path = Path(filepath)
    if not path.exists():
        print(f"ERROR: File not found: {filepath}")
        sys.exit(2)

    try:
        tree = ET.parse(path)
    except ET.ParseError as e:
        print(f"ERROR: Invalid XML in {filepath}: {e}")
        sys.exit(2)

    root = tree.getroot()

    # Parse testsuite(s)
    tests = []
    suites = root.findall(".//testcase")
    for tc in suites:
        name = tc.get("name", "")
        classname = tc.get("classname", "")
        time_val = float(tc.get("time", "0"))

        status = "passed"
        message = ""
        if tc.find("failure") is not None:
            status = "failed"
            message = tc.find("failure").get("message", "")
        elif tc.find("error") is not None:
            status = "error"
            message = tc.find("error").get("message", "")
        elif tc.find("skipped") is not None:
            status = "skipped"
            message = tc.find("skipped").get("message", "")

        tests.append({
            "name": name,
            "classname": classname,
            "fullname": f"{classname}::{name}" if classname else name,
            "time": time_val,
            "status": status,
            "message": message,
        })

    # Parse totals from testsuite attributes
    testsuite = root if root.tag == "testsuite" else root.find("testsuite")
    totals = {}
    if testsuite is not None:
        totals = {
            "tests": int(testsuite.get("tests", 0)),
            "errors": int(testsuite.get("errors", 0)),
            "failures": int(testsuite.get("failures", 0)),
            "skipped": int(testsuite.get("skipped", 0)),
            "time": float(testsuite.get("time", 0)),
        }
        totals["passed"] = (
            totals["tests"] - totals["errors"] - totals["failures"] - totals["skipped"]
        )
    else:
        # Compute from individual tests
        totals = {
            "tests": len(tests),
            "passed": sum(1 for t in tests if t["status"] == "passed"),
            "failures": sum(1 for t in tests if t["status"] == "failed"),
            "errors": sum(1 for t in tests if t["status"] == "error"),
            "skipped": sum(1 for t in tests if t["status"] == "skipped"),
            "time": sum(t["time"] for t in tests),
        }

    return {"tests": tests, "totals": totals}


def compare(old_data: dict, new_data: dict, max_time_ratio: float) -> dict:
    """Compare two test result reports."""
    old_totals = old_data["totals"]
    new_totals = new_data["totals"]

    old_tests = {t["fullname"]: t for t in old_data["tests"]}
    new_tests = {t["fullname"]: t for t in new_data["tests"]}

    old_names = set(old_tests)
    new_names = set(new_tests)

    added = sorted(new_names - old_names)
    removed = sorted(old_names - new_names)
    common = sorted(old_names & new_names)

    # Find regressions (tests that passed before but fail now)
    regressions = []
    for name in common:
        old_status = old_tests[name]["status"]
        new_status = new_tests[name]["status"]
        if old_status == "passed" and new_status in ("failed", "error"):
            regressions.append({
                "name": name,
                "old_status": old_status,
                "new_status": new_status,
                "message": new_tests[name].get("message", ""),
            })

    # New failures (tests that are new AND failing)
    new_failures = [
        name for name in added
        if new_tests[name]["status"] in ("failed", "error")
    ]

    # Time comparison
    old_time = old_totals["time"]
    new_time = new_totals["time"]
    time_ratio = new_time / old_time if old_time > 0 else 1.0
    time_ok = time_ratio <= max_time_ratio

    has_regression = len(regressions) > 0 or len(removed) > 0

    return {
        "old_totals": old_totals,
        "new_totals": new_totals,
        "added": added,
        "removed": removed,
        "common_count": len(common),
        "regressions": regressions,
        "new_failures": new_failures,
        "time_ratio": round(time_ratio, 2),
        "time_ok": time_ok,
        "max_time_ratio": max_time_ratio,
        "has_regression": has_regression,
    }


def format_markdown(result: dict) -> str:
    """Format comparison result as markdown."""
    lines = []
    lines.append("# Test Results Comparison Report")
    lines.append("")

    verdict = "FAIL" if result["has_regression"] else "PASS"
    lines.append(f"**Verdict**: {verdict}")
    lines.append("")

    # Summary table
    old = result["old_totals"]
    new = result["new_totals"]

    lines.append("## Summary")
    lines.append("")
    lines.append("| Metric | Old | New | Diff |")
    lines.append("|--------|-----|-----|------|")

    for key in ["tests", "passed", "failures", "errors", "skipped"]:
        old_val = old.get(key, 0)
        new_val = new.get(key, 0)
        diff = new_val - old_val
        sign = "+" if diff > 0 else ""
        lines.append(f"| {key.title()} | {old_val} | {new_val} | {sign}{diff} |")

    old_time = old.get("time", 0)
    new_time = new.get("time", 0)
    lines.append(
        f"| Time | {old_time:.2f}s | {new_time:.2f}s "
        f"| x{result['time_ratio']:.2f} {'OK' if result['time_ok'] else 'SLOW'} |"
    )
    lines.append("")

    # Test coverage
    lines.append("## Test Set Changes")
    lines.append("")
    lines.append(f"- **Common tests**: {result['common_count']}")
    lines.append(f"- **Added tests**: {len(result['added'])}")
    lines.append(f"- **Removed tests**: {len(result['removed'])}")
    lines.append("")

    if result["added"]:
        lines.append("### Added Tests")
        lines.append("")
        for name in result["added"][:20]:
            lines.append(f"- `{name}`")
        if len(result["added"]) > 20:
            lines.append(f"- ... and {len(result['added']) - 20} more")
        lines.append("")

    if result["removed"]:
        lines.append("### Removed Tests (REGRESSION)")
        lines.append("")
        for name in result["removed"][:20]:
            lines.append(f"- `{name}`")
        if len(result["removed"]) > 20:
            lines.append(f"- ... and {len(result['removed']) - 20} more")
        lines.append("")

    # Regressions
    if result["regressions"]:
        lines.append("## Regressions (Previously Passing, Now Failing)")
        lines.append("")
        for reg in result["regressions"]:
            lines.append(f"- `{reg['name']}`: {reg['old_status']} -> {reg['new_status']}")
            if reg["message"]:
                lines.append(f"  - {reg['message'][:200]}")
        lines.append("")

    if result["new_failures"]:
        lines.append("## New Failing Tests")
        lines.append("")
        for name in result["new_failures"]:
            lines.append(f"- `{name}`")
        lines.append("")

    return "\n".join(lines)


def main():
    args = sys.argv[1:]

    max_time_ratio = 1.5
    output_file = None
    positional = []

    i = 0
    while i < len(args):
        if args[i] == "--max-time-ratio" and i + 1 < len(args):
            max_time_ratio = float(args[i + 1])
            i += 2
        elif args[i] == "--output" and i + 1 < len(args):
            output_file = args[i + 1]
            i += 2
        elif args[i] == "--help":
            print(__doc__)
            sys.exit(0)
        else:
            positional.append(args[i])
            i += 1

    if len(positional) != 2:
        print("Usage: compare_test_results.py <old_results.xml> <new_results.xml>")
        print("       compare_test_results.py old.xml new.xml --max-time-ratio 1.5")
        print("       compare_test_results.py old.xml new.xml --output report.md")
        sys.exit(2)

    old_file, new_file = positional

    old_data = load_junit_xml(old_file)
    new_data = load_junit_xml(new_file)
    result = compare(old_data, new_data, max_time_ratio)

    report = format_markdown(result)

    if output_file:
        Path(output_file).write_text(report)
        print(f"Report written to {output_file}")
    else:
        print(report)

    if result["has_regression"]:
        count = len(result["regressions"]) + len(result["removed"])
        print(f"\nFAILED: {count} regression(s) detected")
        sys.exit(1)
    else:
        print("\nPASSED: No test regressions")
        sys.exit(0)


if __name__ == "__main__":
    main()
