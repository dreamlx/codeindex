#!/usr/bin/env python3
"""Compare two pytest coverage JSON reports and detect regressions.

Reads two coverage.json files (generated by pytest-cov --cov-report=json),
calculates per-module coverage diff, and generates a markdown report.

Usage:
    python compare_coverage.py old_coverage.json new_coverage.json
    python compare_coverage.py old.json new.json --threshold 1.0
    python compare_coverage.py old.json new.json --output report.md

Exit codes:
    0 - No regression (new coverage >= old - threshold)
    1 - Regression detected (some module dropped > threshold)
    2 - Input error (file not found, invalid JSON)

Note: This is a CLI tool - print() statements are intentional for user output.
"""
# ruff: noqa: T201

import json
import sys
from pathlib import Path


def load_coverage(filepath: str) -> dict:
    """Load and validate a pytest-cov JSON report."""
    path = Path(filepath)
    if not path.exists():
        print(f"ERROR: File not found: {filepath}")
        sys.exit(2)

    try:
        data = json.loads(path.read_text())
    except json.JSONDecodeError as e:
        print(f"ERROR: Invalid JSON in {filepath}: {e}")
        sys.exit(2)

    if "files" not in data:
        print(f"ERROR: No 'files' key in {filepath}. Is this a pytest-cov JSON report?")
        sys.exit(2)

    return data


def extract_module_coverage(data: dict) -> dict[str, float]:
    """Extract per-module coverage percentages from coverage data."""
    modules = {}
    for filepath, file_data in data.get("files", {}).items():
        summary = file_data.get("summary", {})
        pct = summary.get("percent_covered", 0.0)
        modules[filepath] = round(pct, 2)
    return modules


def compare(old_data: dict, new_data: dict, threshold: float) -> dict:
    """Compare two coverage reports and return comparison results."""
    old_modules = extract_module_coverage(old_data)
    new_modules = extract_module_coverage(new_data)

    old_total = old_data.get("totals", {}).get("percent_covered", 0.0)
    new_total = new_data.get("totals", {}).get("percent_covered", 0.0)

    all_modules = sorted(set(old_modules) | set(new_modules))

    rows = []
    regressions = []

    for module in all_modules:
        old_pct = old_modules.get(module, 0.0)
        new_pct = new_modules.get(module, 0.0)
        diff = round(new_pct - old_pct, 2)

        status = "NEW" if module not in old_modules else (
            "REMOVED" if module not in new_modules else (
                "REGRESSION" if diff < -threshold else (
                    "IMPROVED" if diff > 0.5 else "OK"
                )
            )
        )

        rows.append({
            "module": module,
            "old": old_pct,
            "new": new_pct,
            "diff": diff,
            "status": status,
        })

        if status == "REGRESSION":
            regressions.append(module)

    return {
        "old_total": round(old_total, 2),
        "new_total": round(new_total, 2),
        "total_diff": round(new_total - old_total, 2),
        "rows": rows,
        "regressions": regressions,
        "threshold": threshold,
    }


def format_markdown(result: dict) -> str:
    """Format comparison result as markdown."""
    lines = []
    lines.append("# Coverage Comparison Report")
    lines.append("")

    # Summary
    old_t = result["old_total"]
    new_t = result["new_total"]
    diff_t = result["total_diff"]
    sign = "+" if diff_t >= 0 else ""
    verdict = "PASS" if not result["regressions"] else "FAIL"

    lines.append(f"**Verdict**: {verdict}")
    lines.append(f"**Threshold**: {result['threshold']}%")
    lines.append("")
    lines.append("## Summary")
    lines.append("")
    lines.append("| Metric | Old | New | Diff |")
    lines.append("|--------|-----|-----|------|")
    lines.append(f"| **Total Coverage** | {old_t:.1f}% | {new_t:.1f}% | {sign}{diff_t:.1f}% |")
    lines.append("")

    # Per-module table
    lines.append("## Per-Module Comparison")
    lines.append("")
    lines.append("| Module | Old | New | Diff | Status |")
    lines.append("|--------|-----|-----|------|--------|")

    for row in result["rows"]:
        mod = row["module"]
        # Shorten module path for readability
        if len(mod) > 50:
            mod = "..." + mod[-47:]
        sign = "+" if row["diff"] >= 0 else ""
        status_icon = {
            "OK": "",
            "IMPROVED": "+",
            "REGRESSION": "REGRESSION",
            "NEW": "NEW",
            "REMOVED": "REMOVED",
        }.get(row["status"], row["status"])
        lines.append(
            f"| `{mod}` | {row['old']:.1f}% | {row['new']:.1f}% "
            f"| {sign}{row['diff']:.1f}% | {status_icon} |"
        )

    lines.append("")

    # Regressions detail
    if result["regressions"]:
        lines.append("## Regressions Detected")
        lines.append("")
        for mod in result["regressions"]:
            row = next(r for r in result["rows"] if r["module"] == mod)
            lines.append(
                f"- `{mod}`: {row['old']:.1f}% -> {row['new']:.1f}% ({row['diff']:.1f}%)"
            )
        lines.append("")

    return "\n".join(lines)


def main():
    args = sys.argv[1:]

    # Parse arguments
    threshold = 1.0
    output_file = None
    positional = []

    i = 0
    while i < len(args):
        if args[i] == "--threshold" and i + 1 < len(args):
            threshold = float(args[i + 1])
            i += 2
        elif args[i] == "--output" and i + 1 < len(args):
            output_file = args[i + 1]
            i += 2
        elif args[i] == "--help":
            print(__doc__)
            sys.exit(0)
        else:
            positional.append(args[i])
            i += 1

    if len(positional) != 2:
        print("Usage: compare_coverage.py <old_coverage.json> <new_coverage.json>")
        print("       compare_coverage.py old.json new.json --threshold 1.0")
        print("       compare_coverage.py old.json new.json --output report.md")
        sys.exit(2)

    old_file, new_file = positional

    # Load and compare
    old_data = load_coverage(old_file)
    new_data = load_coverage(new_file)
    result = compare(old_data, new_data, threshold)

    # Output
    report = format_markdown(result)

    if output_file:
        Path(output_file).write_text(report)
        print(f"Report written to {output_file}")
    else:
        print(report)

    # Exit code
    if result["regressions"]:
        print(f"\nFAILED: {len(result['regressions'])} module(s) regressed > {threshold}%")
        sys.exit(1)
    else:
        print(f"\nPASSED: No coverage regression (threshold: {threshold}%)")
        sys.exit(0)


if __name__ == "__main__":
    main()
